import numpy as  np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow import keras
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate synthetic dataset
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2,random_state=42)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)

# Define a neural network model with dropout
def create_dropout_model(dropout_rate=0.2):
    model = Sequential([
        Dense(64, input_shape=(X_train.shape[1],), activation='relu'),
        Dropout(dropout_rate),
        Dense(32, activation='relu'),
        Dropout(dropout_rate),
        Dense(1, activation='sigmoid')
    ])
    return model

# Compile model with Adam optimizer and binary crossentropy loss
dropout_model = create_dropout_model(dropout_rate=0.2)
dropout_model.compile(optimizer='adam', loss='binary_crossentropy',metrics=['accuracy'])

# Train model with dropout
dropout_history = dropout_model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))

# Train model with dropout
history = dropout_model.fit(X_train, y_train, epochs=50,
                            batch_size=32,
                            validation_data=(X_test, y_test), verbose=0)

# Early Stopping
early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss',
                                               patience=5,
                                               restore_best_weights=True)

history = dropout_model.fit(X_train, y_train, epochs=20, validation_split=0.2,
                            callbacks=[early_stopping])

early_stopping_epoch = np.argmin(history.history['val_loss']) + 1
print("\nEarly stopping occurred at epoch:", early_stopping_epoch)


import matplotlib.pyplot as plt

plt.figure(figsize=(10, 6))
plt.plot(dropout_history.history['accuracy'], label='Dropout Training Accuracy')
plt.plot(dropout_history.history['val_accuracy'], label='DO Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend()
plt.grid(True)
plt.show()

# Plotting the training and validation loss
plt.plot(history.history['loss'], label='Training loss')
plt.plot(history.history['val_loss'], label='Validation loss')
plt.xlabel('Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.axvline(x=early_stopping_epoch, color='r', linestyle='--', label='Early Stopping Epoch')
plt.legend()
plt.show()




early_stopping_epoch = np.argmin(history.history['val_loss']) + 1
print("\nEarly stopping occurred at epoch:", early_stopping_epoch)
